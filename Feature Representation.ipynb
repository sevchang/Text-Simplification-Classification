{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infinite-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textstat\n",
    "import syllables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "small-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy english text processing pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "df1 = pd.read_csv('WikiLarge_Train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-midwest",
   "metadata": {},
   "source": [
    "### Spacy NLP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appointed-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step, tokenize and lemmatize the words\n",
    "# also filtered out the non-stopwords, since these words/lemmas should be much more significant to the readability\n",
    "# of a sentence\n",
    "\n",
    "tokens = []\n",
    "lemmas = []\n",
    "nonstop = []\n",
    "nonstop_lem = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    doc = nlp(df1['original_text'][i])\n",
    "\n",
    "    toks = []\n",
    "    lem = []\n",
    "    non_stop = []\n",
    "    non_stop_lem = []\n",
    "\n",
    "    for token in doc: \n",
    "        if token.is_punct == False:\n",
    "            toks.append(token.text)\n",
    "            lem.append(token.lemma_)\n",
    "\n",
    "            if token.is_stop == False:\n",
    "                non_stop.append(token.text)\n",
    "                non_stop_lem.append(token.lemma_)\n",
    "\n",
    "    tokens.append(toks)\n",
    "    lemmas.append(lem)\n",
    "    nonstop.append(non_stop)\n",
    "    nonstop_lem.append(non_stop_lem)\n",
    "\n",
    "df1['tokens'] = pd.Series(tokens)                              # list of tokens\n",
    "df1['lemmas'] = pd.Series(lemmas)                              # list of lemmas\n",
    "df1['non_stop_tokens'] = pd.Series(nonstop)                    # list of non-stopword tokens\n",
    "df1['non_stop_lems'] = pd.Series(nonstop_lem)                  # list of non-stopword lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "anticipated-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the alphabetic/digit/punctuation/stopword tokens count of a sentence\n",
    "\n",
    "alphas = []\n",
    "digits = []\n",
    "Punc = []\n",
    "Stop = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    doc = nlp(df1['original_text'][i])\n",
    "\n",
    "    alpha = 0\n",
    "    digit = 0\n",
    "    punct = 0\n",
    "    stop = 0\n",
    "\n",
    "    for token in doc: \n",
    "        if token.is_punct:\n",
    "            punct += 1\n",
    "        if token.is_alpha:\n",
    "            alpha +=1\n",
    "        if token.is_digit:\n",
    "            digit += 1\n",
    "        if token.is_stop:\n",
    "            stop += 1\n",
    "\n",
    "    alphas.append(alpha)\n",
    "    digits.append(digit)\n",
    "    Punc.append(punct)\n",
    "    Stop.append(stop)\n",
    "\n",
    "df1['alpha_count'] = pd.Series(alphas)         # alphabetic token count\n",
    "df1['digit_count'] = pd.Series(digits)         # digital token count\n",
    "df1['punc_count'] = pd.Series(Punc)            # punctuation token count   (in Spacy pipeline, punctuation itself is a token)\n",
    "df1['stopword_count'] = pd.Series(Stop)        # stopword token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "returning-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use POS tagging to get the info of pronouns, nouns, verbs, adjs of a sentence\n",
    "\n",
    "pos_lst = ['PROPN','VERB','NOUN','ADJ']\n",
    "\n",
    "nouns = []\n",
    "prons = []\n",
    "non_stop_N = []\n",
    "adjs = []\n",
    "non_stop_ADJ = []\n",
    "vcount = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    doc = nlp(df1['original_text'][i])\n",
    "\n",
    "    pns = []\n",
    "    nns = []\n",
    "    non_stop_n = []\n",
    "    adj = []\n",
    "    non_stop_adj = []\n",
    "    v = 0\n",
    "\n",
    "    for token in doc: \n",
    "        if token.pos_ in pos_lst:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                nns.append(token.text)\n",
    "                if token.is_stop == False:\n",
    "                    non_stop_n.append(token.text)\n",
    "            if token.pos_ == 'PROPN':\n",
    "                pns.append(token.text)\n",
    "            if token.pos_ == 'ADJ':\n",
    "                adj.append(token.text)\n",
    "                if token.is_stop == False:\n",
    "                    non_stop_adj.append(token.text)\n",
    "            if token.pos_ == 'VERB':\n",
    "                v += 1\n",
    "\n",
    "    nouns.append(nns)\n",
    "    prons.append(pns)\n",
    "    non_stop_N.append(non_stop_n)\n",
    "    adjs.append(adj)\n",
    "    non_stop_ADJ.append(non_stop_adj)\n",
    "    vcount.append(v)\n",
    "\n",
    "df1['nouns'] = pd.Series(nouns)                                   # list of noun tokens\n",
    "df1['pronouns'] = pd.Series(prons)                                # list of pronouns (might be a multi-word chunk)\n",
    "df1['non_stop_nouns'] = pd.Series(non_stop_N)                     # list of non-stopword nouns\n",
    "df1['adjs'] = pd.Series(adjs)                                     # list of adjectives\n",
    "df1['non_stop_adj'] = pd.Series(non_stop_ADJ)                     # list of non-stopword adjectives\n",
    "df1['verb_count'] = pd.Series(vcount)                             # count of verbs in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elect-ordering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>non_stop_tokens</th>\n",
       "      <th>non_stop_lems</th>\n",
       "      <th>alpha_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>nouns</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>non_stop_nouns</th>\n",
       "      <th>adjs</th>\n",
       "      <th>non_stop_adj</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>entities</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>noun_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[There, is, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[there, be, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[manuscript, evidence, Austen, continued, work...</td>\n",
       "      <td>[manuscript, evidence, Austen, continue, work,...</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>[evidence, pieces, period, niece, nephew, addi...</td>\n",
       "      <td>[Austen, Anna, James, Edward, Austen]</td>\n",
       "      <td>[evidence, pieces, period, niece, nephew, addi...</td>\n",
       "      <td>[manuscript, further]</td>\n",
       "      <td>[manuscript]</td>\n",
       "      <td>4</td>\n",
       "      <td>[Austen, the period 1809 â '' 11, Anna, James ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[manuscript evidence, Austen, these pieces, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[In, a, remarkable, comparative, analysis, Man...</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, man...</td>\n",
       "      <td>[remarkable, comparative, analysis, Mandaean, ...</td>\n",
       "      <td>[remarkable, comparative, analysis, mandaean, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>[analysis, scholar, texts]</td>\n",
       "      <td>[Säve, Söderberg, Mani, Psalms, Thomas]</td>\n",
       "      <td>[analysis, scholar, texts]</td>\n",
       "      <td>[remarkable, comparative, Mandaean, related, M...</td>\n",
       "      <td>[remarkable, comparative, Mandaean, related, M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Mandaean, Säve-Söderberg, Mani, Thomas, Manda...</td>\n",
       "      <td>2</td>\n",
       "      <td>[a remarkable comparative analysis, Mandaean s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before, Persephone, was, released, to, Hermes...</td>\n",
       "      <td>[before, Persephone, be, release, to, Hermes, ...</td>\n",
       "      <td>[Persephone, released, Hermes, sent, retrieve,...</td>\n",
       "      <td>[Persephone, release, Hermes, send, retrieve, ...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>[pomegranate, seeds, telling, underworld, peri...</td>\n",
       "      <td>[Persephone, Hermes, Hades, -LRB-, -RRB-]</td>\n",
       "      <td>[pomegranate, seeds, telling, underworld, peri...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>[Persephone, Hermes, six, three, a period each...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Persephone, Hermes, who, her, Hades, her, pom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Cogeneration, plants, are, commonly, found, i...</td>\n",
       "      <td>[cogeneration, plant, be, commonly, find, in, ...</td>\n",
       "      <td>[Cogeneration, plants, commonly, found, distri...</td>\n",
       "      <td>[cogeneration, plant, commonly, find, district...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>[Cogeneration, plants, district, heating, syst...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Cogeneration, plants, district, heating, syst...</td>\n",
       "      <td>[thermal, industrial, large]</td>\n",
       "      <td>[thermal, industrial, large]</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[Cogeneration plants, district heating systems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Geneva, -LRB-, -RRB-, is, the, second, most, ...</td>\n",
       "      <td>[Geneva, -LRB-, -RRB-, be, the, second, most, ...</td>\n",
       "      <td>[Geneva, -LRB-, -RRB-, second, populous, city,...</td>\n",
       "      <td>[Geneva, -LRB-, -RRB-, second, populous, city,...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>[city, city, part]</td>\n",
       "      <td>[Geneva, -LRB-, -RRB-, Switzerland, -LRB-, Zür...</td>\n",
       "      <td>[city, city]</td>\n",
       "      <td>[populous, populous]</td>\n",
       "      <td>[populous, populous]</td>\n",
       "      <td>1</td>\n",
       "      <td>[Geneva -LRB-, second, Switzerland, Zürich, Fr...</td>\n",
       "      <td>4</td>\n",
       "      <td>[-RRB-, the second-most-populous city, Switzer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  label  \\\n",
       "0  There is manuscript evidence that Austen conti...      1   \n",
       "1  In a remarkable comparative analysis , Mandaea...      1   \n",
       "2  Before Persephone was released to Hermes , who...      1   \n",
       "3  Cogeneration plants are commonly found in dist...      1   \n",
       "4  Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [There, is, manuscript, evidence, that, Austen...   \n",
       "1  [In, a, remarkable, comparative, analysis, Man...   \n",
       "2  [Before, Persephone, was, released, to, Hermes...   \n",
       "3  [Cogeneration, plants, are, commonly, found, i...   \n",
       "4  [Geneva, -LRB-, -RRB-, is, the, second, most, ...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [there, be, manuscript, evidence, that, Austen...   \n",
       "1  [in, a, remarkable, comparative, analysis, man...   \n",
       "2  [before, Persephone, be, release, to, Hermes, ...   \n",
       "3  [cogeneration, plant, be, commonly, find, in, ...   \n",
       "4  [Geneva, -LRB-, -RRB-, be, the, second, most, ...   \n",
       "\n",
       "                                     non_stop_tokens  \\\n",
       "0  [manuscript, evidence, Austen, continued, work...   \n",
       "1  [remarkable, comparative, analysis, Mandaean, ...   \n",
       "2  [Persephone, released, Hermes, sent, retrieve,...   \n",
       "3  [Cogeneration, plants, commonly, found, distri...   \n",
       "4  [Geneva, -LRB-, -RRB-, second, populous, city,...   \n",
       "\n",
       "                                       non_stop_lems  alpha_count  \\\n",
       "0  [manuscript, evidence, Austen, continue, work,...           35   \n",
       "1  [remarkable, comparative, analysis, mandaean, ...           21   \n",
       "2  [Persephone, release, Hermes, send, retrieve, ...           40   \n",
       "3  [cogeneration, plant, commonly, find, district...           32   \n",
       "4  [Geneva, -LRB-, -RRB-, second, populous, city,...           25   \n",
       "\n",
       "   digit_count  punc_count  stopword_count  \\\n",
       "0            3           5              18   \n",
       "1            0           3               7   \n",
       "2            0           4              23   \n",
       "3            0           7               5   \n",
       "4            0          11              13   \n",
       "\n",
       "                                               nouns  \\\n",
       "0  [evidence, pieces, period, niece, nephew, addi...   \n",
       "1                         [analysis, scholar, texts]   \n",
       "2  [pomegranate, seeds, telling, underworld, peri...   \n",
       "3  [Cogeneration, plants, district, heating, syst...   \n",
       "4                                 [city, city, part]   \n",
       "\n",
       "                                            pronouns  \\\n",
       "0              [Austen, Anna, James, Edward, Austen]   \n",
       "1            [Säve, Söderberg, Mani, Psalms, Thomas]   \n",
       "2          [Persephone, Hermes, Hades, -LRB-, -RRB-]   \n",
       "3                                                 []   \n",
       "4  [Geneva, -LRB-, -RRB-, Switzerland, -LRB-, Zür...   \n",
       "\n",
       "                                      non_stop_nouns  \\\n",
       "0  [evidence, pieces, period, niece, nephew, addi...   \n",
       "1                         [analysis, scholar, texts]   \n",
       "2  [pomegranate, seeds, telling, underworld, peri...   \n",
       "3  [Cogeneration, plants, district, heating, syst...   \n",
       "4                                       [city, city]   \n",
       "\n",
       "                                                adjs  \\\n",
       "0                              [manuscript, further]   \n",
       "1  [remarkable, comparative, Mandaean, related, M...   \n",
       "2                                                 []   \n",
       "3                       [thermal, industrial, large]   \n",
       "4                               [populous, populous]   \n",
       "\n",
       "                                        non_stop_adj  verb_count  \\\n",
       "0                                       [manuscript]           4   \n",
       "1  [remarkable, comparative, Mandaean, related, M...           1   \n",
       "2                                                 []           8   \n",
       "3                       [thermal, industrial, large]           2   \n",
       "4                               [populous, populous]           1   \n",
       "\n",
       "                                            entities  entity_type  \\\n",
       "0  [Austen, the period 1809 â '' 11, Anna, James ...            2   \n",
       "1  [Mandaean, Säve-Söderberg, Mani, Thomas, Manda...            2   \n",
       "2  [Persephone, Hermes, six, three, a period each...            4   \n",
       "3                                                 []            0   \n",
       "4  [Geneva -LRB-, second, Switzerland, Zürich, Fr...            4   \n",
       "\n",
       "                                         noun_chunks  \n",
       "0  [manuscript evidence, Austen, these pieces, th...  \n",
       "1  [a remarkable comparative analysis, Mandaean s...  \n",
       "2  [Persephone, Hermes, who, her, Hades, her, pom...  \n",
       "3  [Cogeneration plants, district heating systems...  \n",
       "4  [-RRB-, the second-most-populous city, Switzer...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spacy pipeline is very powerful in text processing, can even get us\n",
    "# the entities or noun chunks in a sentence\n",
    "\n",
    "entity = []\n",
    "entity_type = []\n",
    "noun_chunk = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    doc = nlp(df1['original_text'][i])\n",
    "\n",
    "    ents = []\n",
    "    ent_type =[]\n",
    "    chunks = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        ents.append(ent.text)\n",
    "        if ent.label_ not in ent_type:\n",
    "            ent_type.append(ent.label_)\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunks.append(chunk.text)\n",
    "\n",
    "    entity.append(ents)\n",
    "    entity_type.append(len(ent_type))\n",
    "    noun_chunk.append(chunks)\n",
    "\n",
    "df1['entities'] = pd.Series(entity)                       # list of entities\n",
    "df1['entity_type'] = pd.Series(entity_type)               # how many different types of entity are in the sentence\n",
    "df1['noun_chunks'] = pd.Series(noun_chunk)                # list of noun chunks\n",
    "\n",
    "\n",
    "# after all those processing, let's see what the data look like now :\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-retro",
   "metadata": {},
   "source": [
    "### Self-defined features\n",
    "\n",
    "Start by defining some functions that can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "champion-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(text):\n",
    "    count = 0\n",
    "    for w in text: \n",
    "        if w.isalpha():\n",
    "            count += 1\n",
    "    return count\n",
    "    # return the alphabetic part length of a string\n",
    "\n",
    "def num(text):\n",
    "    count = 0\n",
    "    for w in text: \n",
    "        if w.isnumeric():\n",
    "            count += 1\n",
    "    return count\n",
    "    # return the numeric part length of a string\n",
    "\n",
    "\n",
    "def unique(list_):\n",
    "    result = len(set(list_))\n",
    "    return result\n",
    "    # return the count of unique values within a list\n",
    "\n",
    "def count_sylla(list_):\n",
    "    result = []\n",
    "    \n",
    "    if len(list_) == 0:\n",
    "        return result\n",
    "    else: \n",
    "        for word in list_:\n",
    "            result.append(syllables.estimate(word))\n",
    "        \n",
    "        return result\n",
    "        # return the syllables estimates of a token list as another list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shared-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sylla(list_):\n",
    "    return np.mean(count_sylla(list_))\n",
    "    # return the average syllables of a list (returned from the above function)\n",
    "\n",
    "def count_sylla_over(list_,thres):\n",
    "    result = 0\n",
    "    \n",
    "    if len(list_) == 0:\n",
    "        return result\n",
    "    else: \n",
    "        for word in list_:\n",
    "            if syllables.estimate(word) >= thres:\n",
    "                result += 1\n",
    "        \n",
    "        return result\n",
    "        # return the count of tokens that have a syllable count equal or above the threshold given\n",
    "        \n",
    "def total_sylla(list_): \n",
    "    result = 0\n",
    "    \n",
    "    for w in list_:\n",
    "        result += syllables.estimate(w)\n",
    "        \n",
    "    return result\n",
    "    # return the total syllable count of a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "favorite-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>non_stop_tokens</th>\n",
       "      <th>non_stop_lems</th>\n",
       "      <th>alpha_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>pronoun_ratio</th>\n",
       "      <th>nonstop_noun_ratio</th>\n",
       "      <th>nonstop_adj_ratio</th>\n",
       "      <th>nounchunk_ratio</th>\n",
       "      <th>token_avg_sylla</th>\n",
       "      <th>lemma_avg_sylla</th>\n",
       "      <th>nonstop_avg_sylla</th>\n",
       "      <th>adj_avg_sylla</th>\n",
       "      <th>entity_avg_sylla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[There, is, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[there, be, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[manuscript, evidence, Austen, continued, work...</td>\n",
       "      <td>[manuscript, evidence, Austen, continue, work,...</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[In, a, remarkable, comparative, analysis, Man...</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, man...</td>\n",
       "      <td>[remarkable, comparative, analysis, Mandaean, ...</td>\n",
       "      <td>[remarkable, comparative, analysis, mandaean, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>2.045455</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before, Persephone, was, released, to, Hermes...</td>\n",
       "      <td>[before, Persephone, be, release, to, Hermes, ...</td>\n",
       "      <td>[Persephone, released, Hermes, sent, retrieve,...</td>\n",
       "      <td>[Persephone, release, Hermes, send, retrieve, ...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>1.619048</td>\n",
       "      <td>1.547619</td>\n",
       "      <td>2.210526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  label  \\\n",
       "0  There is manuscript evidence that Austen conti...      1   \n",
       "1  In a remarkable comparative analysis , Mandaea...      1   \n",
       "2  Before Persephone was released to Hermes , who...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [There, is, manuscript, evidence, that, Austen...   \n",
       "1  [In, a, remarkable, comparative, analysis, Man...   \n",
       "2  [Before, Persephone, was, released, to, Hermes...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [there, be, manuscript, evidence, that, Austen...   \n",
       "1  [in, a, remarkable, comparative, analysis, man...   \n",
       "2  [before, Persephone, be, release, to, Hermes, ...   \n",
       "\n",
       "                                     non_stop_tokens  \\\n",
       "0  [manuscript, evidence, Austen, continued, work...   \n",
       "1  [remarkable, comparative, analysis, Mandaean, ...   \n",
       "2  [Persephone, released, Hermes, sent, retrieve,...   \n",
       "\n",
       "                                       non_stop_lems  alpha_count  \\\n",
       "0  [manuscript, evidence, Austen, continue, work,...           35   \n",
       "1  [remarkable, comparative, analysis, mandaean, ...           21   \n",
       "2  [Persephone, release, Hermes, send, retrieve, ...           40   \n",
       "\n",
       "   digit_count  punc_count  stopword_count  ... noun_ratio pronoun_ratio  \\\n",
       "0            3           5              18  ...   0.157895      0.131579   \n",
       "1            0           3               7  ...   0.136364      0.227273   \n",
       "2            0           4              23  ...   0.142857      0.119048   \n",
       "\n",
       "  nonstop_noun_ratio nonstop_adj_ratio nounchunk_ratio  token_avg_sylla  \\\n",
       "0           0.157895          0.026316        0.236842         1.500000   \n",
       "1           0.136364          0.227273        0.227273         2.090909   \n",
       "2           0.142857          0.000000        0.309524         1.619048   \n",
       "\n",
       "  lemma_avg_sylla  nonstop_avg_sylla adj_avg_sylla  entity_avg_sylla  \n",
       "0        1.500000           1.800000           2.5               3.0  \n",
       "1        2.045455           2.533333           3.2               2.2  \n",
       "2        1.547619           2.210526           NaN               2.6  \n",
       "\n",
       "[3 rows x 54 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['raw_length'] = df1['original_text'].apply(len)\n",
    "df1['token_count'] = df1['tokens'].apply(len)\n",
    "df1['token_count_raw'] = df1['token_count'] + df1['punc_count'] \n",
    "\n",
    "df1['alpha_ratio'] = df1['alpha_count'] / df1['token_count']\n",
    "df1['digit_ratio'] = df1['digit_count'] / df1['token_count']\n",
    "df1['punc_ratio'] = df1['punc_count'] / df1['token_count_raw']\n",
    "df1['stop_ratio'] = df1['stopword_count'] / df1['token_count']\n",
    "\n",
    "df1['nonstop_count'] = df1['non_stop_tokens'].apply(len)                      # count of non-stopword tokens\n",
    "df1['nonstop_ratio'] = df1['nonstop_count'] / df1['token_count']\n",
    "df1['nonstop_lem_count'] = df1['non_stop_lems'].apply(len)                    # count of non-stopword lemmas\n",
    "df1['nonstop_lem_ratio'] = df1['nonstop_lem_count'] / df1['token_count']\n",
    "\n",
    "df1['unique_token_count'] = df1['tokens'].apply(unique)\n",
    "df1['unique_lemma_count'] = df1['lemmas'].apply(unique)\n",
    "df1['unique_nonstop_token'] = df1['non_stop_tokens'].apply(unique)\n",
    "df1['unique_nonstop_lem'] = df1['non_stop_lems'].apply(unique)\n",
    "\n",
    "df1['unique_word_ratio'] = df1['unique_token_count'] / df1['token_count']\n",
    "df1['unique_lemma_ratio'] = df1['unique_lemma_count'] / df1['token_count']\n",
    "df1['unique_nonstop_ratio'] = df1['unique_nonstop_token'] / df1['token_count']\n",
    "df1['unique_nonstoplem_ratio'] = df1['unique_nonstop_lem'] / df1['token_count']\n",
    "\n",
    "df1['noun_count'] = df1['nouns'].apply(len)\n",
    "df1['pronoun_count'] = df1['pronouns'].apply(len)\n",
    "df1['nonstop_noun_count'] = df1['non_stop_nouns'].apply(len)\n",
    "df1['nonstop_adj_count'] = df1['non_stop_adj'].apply(len)\n",
    "df1['entity_count'] = df1['entities'].apply(len)\n",
    "df1['nounchunk_count'] = df1['noun_chunks'].apply(len)\n",
    "\n",
    "df1['noun_ratio'] = df1['noun_count'] / df1['token_count']\n",
    "df1['pronoun_ratio'] = df1['pronoun_count'] / df1['token_count']\n",
    "df1['nonstop_noun_ratio'] = df1['nonstop_noun_count'] / df1['token_count']\n",
    "df1['nonstop_adj_ratio'] = df1['nonstop_adj_count'] / df1['token_count']\n",
    "df1['nounchunk_ratio'] = df1['nounchunk_count'] / df1['token_count']\n",
    "\n",
    "df1['token_avg_sylla'] = df1['tokens'].apply(avg_sylla)\n",
    "df1['lemma_avg_sylla'] = df1['lemmas'].apply(avg_sylla)\n",
    "df1['nonstop_avg_sylla'] = df1['non_stop_tokens'].apply(avg_sylla)\n",
    "df1['adj_avg_sylla'] = df1['adjs'].apply(avg_sylla)\n",
    "df1['entity_avg_sylla'] = df1['entities'].apply(avg_sylla)\n",
    "\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-burden",
   "metadata": {},
   "source": [
    "### Textstat features\n",
    "\n",
    "Textstat package can help us calculate some textual statistics and readability indexes at a very fast pace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wireless-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some textual statistics\n",
    "\n",
    "df1['char_per_word'] = df1['original_text'].apply(textstat.avg_character_per_word)\n",
    "df1['sylla_count'] = df1['original_text'].apply(textstat.syllable_count)\n",
    "df1['lexicon_count'] = df1['original_text'].apply(textstat.lexicon_count)\n",
    "df1['char_count'] = df1['original_text'].apply(textstat.char_count)\n",
    "df1['letter_count'] = df1['original_text'].apply(textstat.letter_count)\n",
    "df1['poly_sylla'] = df1['original_text'].apply(textstat.polysyllabcount)            # count of 2+ syllable words\n",
    "df1['mono_sylla'] = df1['original_text'].apply(textstat.monosyllabcount)            # count of 3+ syllable words\n",
    "\n",
    "# readability indexes below\n",
    "\n",
    "df1['flesch_ease'] = df1['original_text'].apply(textstat.flesch_reading_ease)\n",
    "df1['smog_index'] = df1['original_text'].apply(textstat.smog_index)\n",
    "df1['flesch_grad'] = df1['original_text'].apply(textstat.flesch_kincaid_grade)\n",
    "df1['coleman_index'] = df1['original_text'].apply(textstat.coleman_liau_index)\n",
    "df1['automated_index'] = df1['original_text'].apply(textstat.automated_readability_index)\n",
    "df1['dalechall_score'] = df1['original_text'].apply(textstat.dale_chall_readability_score)\n",
    "df1['difficult_words'] = df1['original_text'].apply(textstat.difficult_words)\n",
    "df1['linsear_formula'] = df1['original_text'].apply(textstat.linsear_write_formula)\n",
    "df1['gunning_fog'] = df1['original_text'].apply(textstat.gunning_fog)\n",
    "df1['text_standard'] = df1['original_text'].apply(textstat.text_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "greek-wrapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>non_stop_tokens</th>\n",
       "      <th>non_stop_lems</th>\n",
       "      <th>alpha_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_grad</th>\n",
       "      <th>coleman_index</th>\n",
       "      <th>automated_index</th>\n",
       "      <th>dalechall_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>text_standard</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[There, is, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[there, be, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[manuscript, evidence, Austen, continued, work...</td>\n",
       "      <td>[manuscript, evidence, Austen, continue, work,...</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>8.77</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.59</td>\n",
       "      <td>7</td>\n",
       "      <td>26.5</td>\n",
       "      <td>20.46</td>\n",
       "      <td>16th and 17th grade</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[In, a, remarkable, comparative, analysis, Man...</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, man...</td>\n",
       "      <td>[remarkable, comparative, analysis, Mandaean, ...</td>\n",
       "      <td>[remarkable, comparative, analysis, mandaean, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.96</td>\n",
       "      <td>9</td>\n",
       "      <td>16.5</td>\n",
       "      <td>17.92</td>\n",
       "      <td>17th and 18th grade</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before, Persephone, was, released, to, Hermes...</td>\n",
       "      <td>[before, Persephone, be, release, to, Hermes, ...</td>\n",
       "      <td>[Persephone, released, Hermes, sent, retrieve,...</td>\n",
       "      <td>[Persephone, release, Hermes, send, retrieve, ...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>10.52</td>\n",
       "      <td>22.3</td>\n",
       "      <td>11.73</td>\n",
       "      <td>9</td>\n",
       "      <td>27.0</td>\n",
       "      <td>20.61</td>\n",
       "      <td>11th and 12th grade</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  label  \\\n",
       "0  There is manuscript evidence that Austen conti...      1   \n",
       "1  In a remarkable comparative analysis , Mandaea...      1   \n",
       "2  Before Persephone was released to Hermes , who...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [There, is, manuscript, evidence, that, Austen...   \n",
       "1  [In, a, remarkable, comparative, analysis, Man...   \n",
       "2  [Before, Persephone, was, released, to, Hermes...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [there, be, manuscript, evidence, that, Austen...   \n",
       "1  [in, a, remarkable, comparative, analysis, man...   \n",
       "2  [before, Persephone, be, release, to, Hermes, ...   \n",
       "\n",
       "                                     non_stop_tokens  \\\n",
       "0  [manuscript, evidence, Austen, continued, work...   \n",
       "1  [remarkable, comparative, analysis, Mandaean, ...   \n",
       "2  [Persephone, released, Hermes, sent, retrieve,...   \n",
       "\n",
       "                                       non_stop_lems  alpha_count  \\\n",
       "0  [manuscript, evidence, Austen, continue, work,...           35   \n",
       "1  [remarkable, comparative, analysis, mandaean, ...           21   \n",
       "2  [Persephone, release, Hermes, send, retrieve, ...           40   \n",
       "\n",
       "   digit_count  punc_count  stopword_count  ... smog_index flesch_grad  \\\n",
       "0            3           5              18  ...        0.0        16.9   \n",
       "1            0           3               7  ...        0.0        15.0   \n",
       "2            0           4              23  ...        0.0        18.5   \n",
       "\n",
       "  coleman_index automated_index dalechall_score  difficult_words  \\\n",
       "0          8.77            19.0           12.59                7   \n",
       "1         18.10            18.7           15.96                9   \n",
       "2         10.52            22.3           11.73                9   \n",
       "\n",
       "  linsear_formula  gunning_fog        text_standard  grade  \n",
       "0            26.5        20.46  16th and 17th grade     16  \n",
       "1            16.5        17.92  17th and 18th grade     17  \n",
       "2            27.0        20.61  11th and 12th grade     11  \n",
       "\n",
       "[3 rows x 72 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn the 'text_standard' variable into numeric feature\n",
    "\n",
    "lst = []\n",
    "\n",
    "for i in df1.text_standard:\n",
    "    first = i.split(' ')[0]\n",
    "    if len(first) == 3:\n",
    "        num = int(first[:1])\n",
    "    else:\n",
    "        num = int(first[:2])\n",
    "    lst.append(num)\n",
    "    \n",
    "df1['grade'] = pd.Series(lst)\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-military",
   "metadata": {},
   "source": [
    "### Incorporate External Resource\n",
    "\n",
    "Start from the dale_chall basic English words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respected-aircraft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'aboard', 'about', 'above ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('dale_chall.txt', 'r')\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "basic = text.split('\\n')\n",
    "basic[0:5]\n",
    "\n",
    "# a list of basic words in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adjacent-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = []\n",
    "lst2 = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    count = 0\n",
    "    non_basic = []\n",
    "    \n",
    "    # iterate through the tokens\n",
    "    for w in df1.tokens[i]:\n",
    "        if w.lower() not in basic:\n",
    "            count += 1\n",
    "            non_basic.append(w)\n",
    "    lst1.append(count)\n",
    "    lst2.append(non_basic)\n",
    "\n",
    "df1['non_basic_tokens'] = pd.Series(lst2)    \n",
    "df1['non_basic_count'] = pd.Series(lst1)\n",
    "df1['non_basic_ratio'] = df1['non_basic_count'] / df1['token_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exclusive-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['nonbasic_avg_sylla'] = df1['non_basic_tokens'].apply(avg_sylla)\n",
    "\n",
    "# how many non_basic tokens are above 5 syllables\n",
    "df1['5sylla_nonbasic'] = df1.apply(lambda row: count_sylla_over(row['non_basic_tokens'],5), axis=1)\n",
    "df1['nonbasic5_ratio'] = df1['5sylla_nonbasic'] / df1['token_count']\n",
    "\n",
    "# the ratio of non-basic/non-stop words avg syllable to tokens average syllable\n",
    "df1['sylla_ratio1'] = df1['nonbasic_avg_sylla'] / df1['token_avg_sylla']\n",
    "df1['sylla_ratio2'] = df1['nonstop_avg_sylla'] / df1['token_avg_sylla']\n",
    "\n",
    "df1['basic_count'] = df1['token_count'] - df1['non_basic_count']\n",
    "df1['basic_ratio'] = df1['basic_count'] / df1['token_count']\n",
    "\n",
    "df1['difficult_ratio'] = df1['difficult_words'] / df1['token_count']\n",
    "df1['diff-basic_ratio'] = df1['difficult_words'] / df1['basic_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-painting",
   "metadata": {},
   "source": [
    "Concreteness Ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "diagnostic-fifth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Conc.SD</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent_known</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>Dom_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roadsweeper</td>\n",
       "      <td>0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traindriver</td>\n",
       "      <td>0</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Bigram  Conc.M  Conc.SD  Unknown  Total  Percent_known  \\\n",
       "0  roadsweeper       0    4.85     0.37        1     27           0.96   \n",
       "1  traindriver       0    4.54     0.71        3     29           0.90   \n",
       "\n",
       "   SUBTLEX Dom_Pos  \n",
       "0        0       0  \n",
       "1        0       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['lemma_set'] = df1['lemmas'].apply(set)\n",
    "\n",
    "dff = pd.read_csv('Concreteness_ratings.csv')\n",
    "\n",
    "word_lst = list(dff.Word.values)\n",
    "conc_lst = list(dff['Conc.M'].values)\n",
    "known_lst = list(dff.Percent_known.values)\n",
    "sub_lst = list(dff.SUBTLEX.values)\n",
    "\n",
    "dff.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mathematical-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "lst1 = []\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    basic_lemma = []\n",
    "    ratings = []\n",
    "    knowns = []\n",
    "    subs = []\n",
    "\n",
    "    for lemma in df1.lemma_set[i]:\n",
    "        if lemma in word_lst:\n",
    "            basic_lemma.append(lemma)\n",
    "            ind = word_lst.index(lemma)\n",
    "            ratings.append(conc_lst[ind])\n",
    "            knowns.append(known_lst[ind])\n",
    "            subs.append(sub_lst[ind])\n",
    "    \n",
    "    lst.append(basic_lemma)\n",
    "    lst1.append(ratings)\n",
    "    lst2.append(knowns)\n",
    "    lst3.append(subs)\n",
    "\n",
    "df1['basic_lemmas'] = pd.Series(lst)               # list of unique lemmas that are listed in the concreteness file\n",
    "df1['lemma_rating'] = pd.Series(lst1)              # list of concreteness rating values of the above basic lemmas\n",
    "df1['known_pct'] = pd.Series(lst2)                 # list of known-percentage values of the above basic lemmas\n",
    "df1['lemma_subtlex'] = pd.Series(lst3)             # list of subtlex values of the above basic lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "enabling-parent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>non_stop_tokens</th>\n",
       "      <th>non_stop_lems</th>\n",
       "      <th>alpha_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>basic_lemmas</th>\n",
       "      <th>lemma_rating</th>\n",
       "      <th>known_pct</th>\n",
       "      <th>lemma_subtlex</th>\n",
       "      <th>basic_lemma_count</th>\n",
       "      <th>basic_lemma_ratio</th>\n",
       "      <th>basic_lemma_ratio1</th>\n",
       "      <th>nonbasic_lemma_count</th>\n",
       "      <th>nonbasic_lemma_ratio</th>\n",
       "      <th>nonbasic_lemma_ratio1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[There, is, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[there, be, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[manuscript, evidence, Austen, continued, work...</td>\n",
       "      <td>[manuscript, evidence, Austen, continue, work,...</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>[continue, the, addition, to, work, make, piec...</td>\n",
       "      <td>[2.36, 1.43, 2.89, 1.55, 3.48, 2.67, 4.14, 2.6...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0,...</td>\n",
       "      <td>[2527, 1501908, 395, 1156570, 40699, 70775, 63...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>8</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[In, a, remarkable, comparative, analysis, Man...</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, man...</td>\n",
       "      <td>[remarkable, comparative, analysis, Mandaean, ...</td>\n",
       "      <td>[remarkable, comparative, analysis, mandaean, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>[related, to, of, comparative, remarkable, ana...</td>\n",
       "      <td>[2.56, 1.55, 1.67, 1.74, 1.89, 2.56, 1.46, 4.9...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0,...</td>\n",
       "      <td>[643, 1156570, 590439, 41, 641, 563, 1041179, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>7</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before, Persephone, was, released, to, Hermes...</td>\n",
       "      <td>[before, Persephone, be, release, to, Hermes, ...</td>\n",
       "      <td>[Persephone, released, Hermes, sent, retrieve,...</td>\n",
       "      <td>[Persephone, release, Hermes, send, retrieve, ...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>[underworld, who, the, six, accord, telling, t...</td>\n",
       "      <td>[2.96, 1.74, 1.43, 3.43, 1.57, 2.45, 1.55, 3.3...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 0.97, 1.0...</td>\n",
       "      <td>[181, 113370, 1501908, 10176, 83, 9730, 115657...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>5</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.119048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  label  \\\n",
       "0  There is manuscript evidence that Austen conti...      1   \n",
       "1  In a remarkable comparative analysis , Mandaea...      1   \n",
       "2  Before Persephone was released to Hermes , who...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [There, is, manuscript, evidence, that, Austen...   \n",
       "1  [In, a, remarkable, comparative, analysis, Man...   \n",
       "2  [Before, Persephone, was, released, to, Hermes...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [there, be, manuscript, evidence, that, Austen...   \n",
       "1  [in, a, remarkable, comparative, analysis, man...   \n",
       "2  [before, Persephone, be, release, to, Hermes, ...   \n",
       "\n",
       "                                     non_stop_tokens  \\\n",
       "0  [manuscript, evidence, Austen, continued, work...   \n",
       "1  [remarkable, comparative, analysis, Mandaean, ...   \n",
       "2  [Persephone, released, Hermes, sent, retrieve,...   \n",
       "\n",
       "                                       non_stop_lems  alpha_count  \\\n",
       "0  [manuscript, evidence, Austen, continue, work,...           35   \n",
       "1  [remarkable, comparative, analysis, mandaean, ...           21   \n",
       "2  [Persephone, release, Hermes, send, retrieve, ...           40   \n",
       "\n",
       "   digit_count  punc_count  stopword_count  ...  \\\n",
       "0            3           5              18  ...   \n",
       "1            0           3               7  ...   \n",
       "2            0           4              23  ...   \n",
       "\n",
       "                                        basic_lemmas  \\\n",
       "0  [continue, the, addition, to, work, make, piec...   \n",
       "1  [related, to, of, comparative, remarkable, ana...   \n",
       "2  [underworld, who, the, six, accord, telling, t...   \n",
       "\n",
       "                                        lemma_rating  \\\n",
       "0  [2.36, 1.43, 2.89, 1.55, 3.48, 2.67, 4.14, 2.6...   \n",
       "1  [2.56, 1.55, 1.67, 1.74, 1.89, 2.56, 1.46, 4.9...   \n",
       "2  [2.96, 1.74, 1.43, 3.43, 1.57, 2.45, 1.55, 3.3...   \n",
       "\n",
       "                                           known_pct  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0,...   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0,...   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 0.97, 1.0...   \n",
       "\n",
       "                                       lemma_subtlex basic_lemma_count  \\\n",
       "0  [2527, 1501908, 395, 1156570, 40699, 70775, 63...                22   \n",
       "1  [643, 1156570, 590439, 41, 641, 563, 1041179, ...                14   \n",
       "2  [181, 113370, 1501908, 10176, 83, 9730, 115657...                29   \n",
       "\n",
       "   basic_lemma_ratio basic_lemma_ratio1  nonbasic_lemma_count  \\\n",
       "0           0.733333           0.578947                     8   \n",
       "1           0.666667           0.636364                     7   \n",
       "2           0.852941           0.690476                     5   \n",
       "\n",
       "  nonbasic_lemma_ratio  nonbasic_lemma_ratio1  \n",
       "0             0.266667               0.210526  \n",
       "1             0.333333               0.318182  \n",
       "2             0.147059               0.119048  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['basic_lemma_count'] = df1['basic_lemmas'].apply(len)\n",
    "\n",
    "# the ratio of the basic lemma count to the total unique lemma count & total tokens count\n",
    "df1['basic_lemma_ratio'] = df1['basic_lemma_count'] / df1['unique_lemma_count']\n",
    "df1['basic_lemma_ratio1'] = df1['basic_lemma_count'] / df1['token_count']\n",
    "\n",
    "df1['nonbasic_lemma_count'] = df1['unique_lemma_count'] - df1['basic_lemma_count']\n",
    "df1['nonbasic_lemma_ratio'] = df1['nonbasic_lemma_count'] / df1['unique_lemma_count']\n",
    "df1['nonbasic_lemma_ratio1'] = df1['nonbasic_lemma_count'] / df1['token_count']\n",
    "\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-yemen",
   "metadata": {},
   "source": [
    "Age of Acquisition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "changed-update",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  \\\n",
       "0         a                    a  20415.27         Article         1      1   \n",
       "1  aardvark             aardvark      0.41            Noun         8      7   \n",
       "\n",
       "   Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0      1                 a     2.89         1.0         2.89             1.0   \n",
       "1      2          aardvark     9.89         1.0         9.89             1.0   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  \n",
       "1           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = pd.read_csv('AoA_words.csv',encoding= 'unicode_escape')\n",
    "\n",
    "word_lst1 = list(ddf.Word.values)\n",
    "aoa_lst = list(ddf['AoA_Kup_lem'].values)\n",
    "known_lst1 = list(ddf.Perc_known_lem.values)\n",
    "\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exclusive-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "lst1 = []\n",
    "lst2 = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    aoa_words = []\n",
    "    ratings = []\n",
    "    knowns = []\n",
    "\n",
    "    for w in df1.tokens[i]:\n",
    "        if w.lower() in word_lst1:\n",
    "            aoa_words.append(w)\n",
    "            ind = word_lst1.index(w.lower())\n",
    "            ratings.append(aoa_lst[ind])\n",
    "            knowns.append(known_lst1[ind])\n",
    "\n",
    "    lst.append(aoa_words)\n",
    "    lst1.append(ratings)\n",
    "    lst2.append(knowns)\n",
    "\n",
    "df1['word_in_AOA'] = pd.Series(lst)                       # words listed in the age of acquisition file\n",
    "df1['word_aoa_scores'] = pd.Series(lst1)                  # list of AoA scores of the above words\n",
    "df1['aoa_known_pct'] = pd.Series(lst2)                    # list of known-percentage values of the above words\n",
    "\n",
    "df1['aoa_count'] = df1['word_in_AOA'].apply(len)\n",
    "df1['aoa_ratio'] = df1['aoa_count'] / df1['token_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-print",
   "metadata": {},
   "source": [
    "### Calculate some summary statistics from the information we extracted above\n",
    "\n",
    "A lot of the columns are just lists of values for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "immediate-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "lst1=[]\n",
    "lst2=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if len(df1['lemma_rating'][i]) == 0:\n",
    "        lst.append(0)\n",
    "        lst1.append(0)\n",
    "        lst2.append(0)\n",
    "    \n",
    "    else: \n",
    "        lst.append(np.mean(df1['lemma_rating'][i]))\n",
    "        lst1.append(np.std(df1['lemma_rating'][i]))\n",
    "        lst2.append(max(df1['lemma_rating'][i]) - min(df1['lemma_rating'][i]))\n",
    "\n",
    "df1['avg_lemma_conc'] = pd.Series(lst)                # the average concreteness rating for the basic lemmas\n",
    "df1['lemma_conc_std'] = pd.Series(lst1)               # the std of the basic lemmas concreteness ratings\n",
    "df1['conc_max-min'] = pd.Series(lst2)                 # the max-min difference for lemma concreteness ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loved-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "lst1=[]\n",
    "lst2=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if len(df1['known_pct'][i]) == 0:\n",
    "        lst.append(0)\n",
    "        lst1.append(0)\n",
    "        lst2.append(0)\n",
    "    \n",
    "    else: \n",
    "        lst.append(np.mean(df1['known_pct'][i]))\n",
    "        lst1.append(np.std(df1['known_pct'][i]))\n",
    "        lst2.append(min(df1['known_pct'][i]))\n",
    "\n",
    "df1['avg_lemma_known'] = pd.Series(lst)               # the average known percentage for the basic lemmas\n",
    "df1['lemma_known_std'] = pd.Series(lst1)              # the std of the basic lemmas known percentage values\n",
    "df1['lemma_min_known'] = pd.Series(lst2)              # the minimum value of the lemma known percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "further-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "lst1=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if len(df1['lemma_subtlex'][i]) == 0:\n",
    "        lst.append(0)\n",
    "        lst1.append(0)\n",
    "    \n",
    "    else: \n",
    "        lst.append(max(df1['lemma_subtlex'][i]))\n",
    "        lst1.append(min(df1['lemma_subtlex'][i]))\n",
    "        \n",
    "df1['lemma_subtlex_max'] = pd.Series(lst)                # the max subtlex value for the basic lemmas\n",
    "df1['lemma_subtlex_min'] = pd.Series(lst1)               # the min subtlex value for the basic lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "regulated-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "lst1=[]\n",
    "lst2=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if len(df1['word_aoa_scores'][i]) == 0:\n",
    "        lst.append(0)\n",
    "        lst1.append(0)\n",
    "        lst2.append(0)\n",
    "    \n",
    "    else: \n",
    "        lst.append(np.mean(df1['word_aoa_scores'][i]))\n",
    "        lst1.append(np.std(df1['word_aoa_scores'][i]))\n",
    "        lst2.append(max(df1['word_aoa_scores'][i]))\n",
    "\n",
    "df1['avg_word_aoa'] = pd.Series(lst)                   # the average age of acquisition score of the words\n",
    "df1['word_aoa_std'] = pd.Series(lst1)                  # the std of the words' AoA scores\n",
    "df1['word_aoa_max'] = pd.Series(lst2)                  # the maximum word AoA score in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "joined-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "lst1=[]\n",
    "lst2=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if len(df1['aoa_known_pct'][i]) == 0:\n",
    "        lst.append(0)\n",
    "        lst1.append(0)\n",
    "        lst2.append(0)\n",
    "    \n",
    "    else: \n",
    "        lst.append(np.mean(df1['aoa_known_pct'][i]))\n",
    "        lst1.append(np.std(df1['aoa_known_pct'][i]))\n",
    "        lst2.append(min(df1['aoa_known_pct'][i]))\n",
    "\n",
    "df1['aoa_known_avg'] = pd.Series(lst)                      # the words' average known percentage \n",
    "df1['aoa_known_std'] = pd.Series(lst1)                     # std of the words' known percentage values \n",
    "df1['aoa_known_min'] = pd.Series(lst2)                     # the minimum of words' known percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "secondary-comparative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>non_stop_tokens</th>\n",
       "      <th>non_stop_lems</th>\n",
       "      <th>alpha_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>lemma_known_std</th>\n",
       "      <th>lemma_min_known</th>\n",
       "      <th>lemma_subtlex_max</th>\n",
       "      <th>lemma_subtlex_min</th>\n",
       "      <th>avg_word_aoa</th>\n",
       "      <th>word_aoa_std</th>\n",
       "      <th>word_aoa_max</th>\n",
       "      <th>aoa_known_avg</th>\n",
       "      <th>aoa_known_std</th>\n",
       "      <th>aoa_known_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[There, is, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[there, be, manuscript, evidence, that, Austen...</td>\n",
       "      <td>[manuscript, evidence, Austen, continued, work...</td>\n",
       "      <td>[manuscript, evidence, Austen, continue, work,...</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1501908</td>\n",
       "      <td>137</td>\n",
       "      <td>5.923793</td>\n",
       "      <td>1.743145</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.992759</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[In, a, remarkable, comparative, analysis, Man...</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, man...</td>\n",
       "      <td>[remarkable, comparative, analysis, Mandaean, ...</td>\n",
       "      <td>[remarkable, comparative, analysis, mandaean, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1156570</td>\n",
       "      <td>41</td>\n",
       "      <td>7.499286</td>\n",
       "      <td>3.002557</td>\n",
       "      <td>11.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before, Persephone, was, released, to, Hermes...</td>\n",
       "      <td>[before, Persephone, be, release, to, Hermes, ...</td>\n",
       "      <td>[Persephone, released, Hermes, sent, retrieve,...</td>\n",
       "      <td>[Persephone, release, Hermes, send, retrieve, ...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028515</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1501908</td>\n",
       "      <td>13</td>\n",
       "      <td>5.322162</td>\n",
       "      <td>1.800710</td>\n",
       "      <td>11.17</td>\n",
       "      <td>0.995946</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  label  \\\n",
       "0  There is manuscript evidence that Austen conti...      1   \n",
       "1  In a remarkable comparative analysis , Mandaea...      1   \n",
       "2  Before Persephone was released to Hermes , who...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [There, is, manuscript, evidence, that, Austen...   \n",
       "1  [In, a, remarkable, comparative, analysis, Man...   \n",
       "2  [Before, Persephone, was, released, to, Hermes...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [there, be, manuscript, evidence, that, Austen...   \n",
       "1  [in, a, remarkable, comparative, analysis, man...   \n",
       "2  [before, Persephone, be, release, to, Hermes, ...   \n",
       "\n",
       "                                     non_stop_tokens  \\\n",
       "0  [manuscript, evidence, Austen, continued, work...   \n",
       "1  [remarkable, comparative, analysis, Mandaean, ...   \n",
       "2  [Persephone, released, Hermes, sent, retrieve,...   \n",
       "\n",
       "                                       non_stop_lems  alpha_count  \\\n",
       "0  [manuscript, evidence, Austen, continue, work,...           35   \n",
       "1  [remarkable, comparative, analysis, mandaean, ...           21   \n",
       "2  [Persephone, release, Hermes, send, retrieve, ...           40   \n",
       "\n",
       "   digit_count  punc_count  stopword_count  ... lemma_known_std  \\\n",
       "0            3           5              18  ...        0.024427   \n",
       "1            0           3               7  ...        0.020064   \n",
       "2            0           4              23  ...        0.028515   \n",
       "\n",
       "  lemma_min_known lemma_subtlex_max lemma_subtlex_min avg_word_aoa  \\\n",
       "0            0.89           1501908               137     5.923793   \n",
       "1            0.93           1156570                41     7.499286   \n",
       "2            0.88           1501908                13     5.322162   \n",
       "\n",
       "   word_aoa_std word_aoa_max  aoa_known_avg aoa_known_std  aoa_known_min  \n",
       "0      1.743145        12.12       0.992759      0.018175           0.94  \n",
       "1      3.002557        11.94       1.000000      0.000000           1.00  \n",
       "2      1.800710        11.17       0.995946      0.013648           0.95  \n",
       "\n",
       "[3 rows x 109 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the data looks like right now\n",
    "\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-wealth",
   "metadata": {},
   "source": [
    "### Other self-defined features \n",
    "\n",
    "Came up with these a bit later during the project, added them to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "assumed-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1=[]\n",
    "lst2=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    text = df1['original_text'][i]\n",
    "    lst1.append(alpha(text))\n",
    "    lst2.append(num(text))\n",
    "\n",
    "df1['alpha_length'] = pd.Series(lst1)\n",
    "df1['num_length'] = pd.Series(lst2)\n",
    "df1['other_length'] = df1['raw_length'] - df1['alpha_length'] - df1['num_length']\n",
    "\n",
    "df1['alpha_ratio']=df1['alpha_length'] / df1['raw_length']\n",
    "df1['num_ratio']=df1['num_length'] / df1['raw_length']\n",
    "df1['other_ratio']=df1['other_length'] / df1['raw_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tested-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of 5+ syllables tokens\n",
    "df1['5_sylla_token'] = df1.apply(lambda row: count_sylla_over(row['tokens'],5), axis=1)\n",
    "df1['5sylla_ratio'] = df1['5_sylla_token'] / df1['token_count']\n",
    "\n",
    "# count of 5+ syllables non-stopword tokens\n",
    "df1['5_sylla_nonstop'] = df1.apply(lambda row: count_sylla_over(row['non_stop_tokens'],5), axis=1)\n",
    "df1['5sylla_nonstop_ratio'] = df1['5_sylla_nonstop'] / df1['token_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fallen-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['poly_ratio'] = df1['poly_sylla'] / df1['token_count']           # ratio of 2+ syllable token\n",
    "df1['mono_ratio'] = df1['mono_sylla'] / df1['token_count']           # ratio of 3+ syllable token\n",
    "\n",
    "# total syllable count for non-stopwords / pronouns / nonstop nouns / nonstop adjectives\n",
    "# entities / noun chunks / non-basic words\n",
    "\n",
    "df1['nonstop_syllables'] = df1.apply(lambda row: total_sylla(row['non_stop_tokens']), axis=1)\n",
    "df1['pron_syllables'] = df1.apply(lambda row: total_sylla(row['pronouns']), axis=1)\n",
    "df1['nonstop_n_syllables'] = df1.apply(lambda row: total_sylla(row['non_stop_nouns']), axis=1)\n",
    "df1['nonstop_adj_syllables'] = df1.apply(lambda row: total_sylla(row['non_stop_adj']), axis=1)\n",
    "df1['entity_syllables'] = df1.apply(lambda row: total_sylla(row['entities']), axis=1)\n",
    "df1['nounchunk_syllables'] = df1.apply(lambda row: total_sylla(row['noun_chunks']), axis=1)\n",
    "df1['nonbasic_syllables'] = df1.apply(lambda row: total_sylla(row['non_basic_tokens']), axis=1)\n",
    "\n",
    "df1['nonstop_sylla_ratio'] = df1['nonstop_syllables'] / df1['sylla_count']\n",
    "df1['pron_sylla_ratio'] = df1['pron_syllables'] / df1['sylla_count']\n",
    "df1['nonstopN_sylla_ratio'] = df1['nonstop_n_syllables'] / df1['sylla_count']\n",
    "df1['nonstopadj_sylla_ratio'] = df1['nonstop_adj_syllables'] / df1['sylla_count']\n",
    "df1['entity_sylla_ratio'] = df1['entity_syllables'] / df1['sylla_count']\n",
    "df1['nounchunk_sylla_ratio'] = df1['nounchunk_syllables'] / df1['sylla_count']\n",
    "df1['nonbasic_sylla_ratio'] = df1['nonbasic_syllables'] / df1['sylla_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-moderator",
   "metadata": {},
   "source": [
    "### Now we're done creating features, save the values as new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "valid-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.drop(columns=['original_text', 'label', 'tokens', 'lemmas', 'non_stop_tokens', \n",
    "                        'non_stop_lems', 'nouns', 'pronouns', 'non_stop_nouns', 'adjs', 'non_stop_adj', \n",
    "                        'entities', 'noun_chunks', 'text_standard', 'non_basic_tokens', 'lemma_set', \n",
    "                        'basic_lemmas', 'lemma_rating', 'known_pct', 'lemma_subtlex', 'word_in_AOA', 'word_aoa_scores', \n",
    "                        'aoa_known_pct'])\n",
    "\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "# all the features (self-defined + textstat), 116 features\n",
    "df2.to_csv('train_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "blocked-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(columns = ['char_per_word', 'sylla_count', 'lexicon_count', 'char_count', 'letter_count', \n",
    "                          'poly_sylla', 'mono_sylla', 'flesch_ease', 'smog_index', 'flesch_grad', \n",
    "                          'coleman_index', 'automated_index', 'dalechall_score', 'difficult_words', \n",
    "                          'linsear_formula', 'gunning_fog', 'grade'])\n",
    "\n",
    "# the self-defined features only\n",
    "df3.to_csv('train_self_feature.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cognitive-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2[['char_per_word', 'sylla_count', 'lexicon_count', 'char_count', 'letter_count', \n",
    "                          'poly_sylla', 'mono_sylla', 'flesch_ease', 'smog_index', 'flesch_grad', \n",
    "                          'coleman_index', 'automated_index', 'dalechall_score', 'difficult_words', \n",
    "                          'linsear_formula', 'gunning_fog', 'grade']]\n",
    "\n",
    "# the textstat features only\n",
    "df4.to_csv('textstat_value.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-protein",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
