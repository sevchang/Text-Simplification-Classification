{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fantastic-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Slow version of gensim.models.doc2vec is being used\n",
      "Slow version of Fasttext is being used\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fuzzy-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "models = list(gensim.downloader.info()['models'].keys())\n",
    "for model in models:\n",
    "    print(model)\n",
    "    \n",
    "# the trained word2vec models in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "common-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the glove-twitter-200 and glove-wiki-gigaword-300 model\n",
    "# many models are too large to load\n",
    "\n",
    "model= gensim.downloader.load('glove-twitter-200')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "df1 = pd.read_csv('WikiLarge_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exotic-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentences first\n",
    "\n",
    "lst1=[]\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    doc = nlp(df1['original_text'][i])\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    lst1.append(tokens)\n",
    "\n",
    "df1['tokens'] = pd.Series(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assured-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_texts, vec_model): \n",
    "\n",
    "    lst = []\n",
    "\n",
    "    # iterate through all the tokenized lists\n",
    "    for l in tokenized_texts:\n",
    "        lst1=[]\n",
    "        \n",
    "        # for each of the tokenized sentence list, iterate through the tokens (words)\n",
    "        # filter the words that are in the trained model\n",
    "        for word in l:\n",
    "            if word.lower() in vec_model.key_to_index:\n",
    "                lst1.append(word.lower())   \n",
    "                \n",
    "        # generate avearge dense vector based on the filtered words\n",
    "        if len(lst1) > 0:\n",
    "            vec = np.mean(vec_model[lst1],axis=0)\n",
    "            lst.append(vec)\n",
    "            \n",
    "        # append a zeros vector if no word is in the trained model\n",
    "        else: \n",
    "            vec = np.zeros(vec_model.vector_size)\n",
    "            lst.append(vec)\n",
    "    \n",
    "    return np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "objective-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416768, 200)\n"
     ]
    }
   ],
   "source": [
    "X_new = generate_dense_features(df1.tokens , model)\n",
    "X_new.shape\n",
    "\n",
    "# X_new is an numpy array, each row is the average dense vector of the sentence in the original train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continuous-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_new)\n",
    "df.to_csv('twitter_vec_dense.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
